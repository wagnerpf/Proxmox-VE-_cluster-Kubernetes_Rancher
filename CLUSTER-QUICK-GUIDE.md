# âš¡ Guia RÃ¡pido: Kubernetes + Rancher no Proxmox VE

> **Deploy completo em 15 minutos** - Cluster Kubernetes empresarial com interface Rancher

## ğŸ¯ **Setup RÃ¡pido - 4 Comandos**

```bash
# 1ï¸âƒ£ Preparar autenticaÃ§Ã£o
ssh-keygen -t rsa -b 4096 -f ~/.ssh/k8s-cluster-key

# 2ï¸âƒ£ Configurar projeto  
cp terraform.tfvars.example terraform.tfvars && nano terraform.tfvars

# 3ï¸âƒ£ InstalaÃ§Ã£o completa
make install

# 4ï¸âƒ£ Acessar Rancher
open https://172.17.176.34:8443  # admin / admin123
```

**â±ï¸ Tempo total:** 15-20 minutos

---

## ğŸ“‹ **PrÃ©-requisitos Essenciais**

### âœ… **Checklist BÃ¡sico**
- [ ] **Proxmox VE** 7.0+ funcionando
- [ ] **Template Ubuntu 22.04** criado no nÃ³ "gardenia"  
- [ ] **Token API** do Proxmox configurado
- [ ] **Terraform + Ansible** instalados na estaÃ§Ã£o
- [ ] **Chaves SSH** geradas e configuradas

### ğŸ”§ **VerificaÃ§Ã£o RÃ¡pida**
```bash
# Testar ferramentas
terraform version  # >= 1.0
ansible --version  # >= 2.12
python3 --version  # >= 3.8

# Testar conectividade Proxmox
curl -k https://cacto.cefetes.br:8006/api2/json/version
```

---

## âš™ï¸ **ConfiguraÃ§Ã£o Essencial**

### ğŸ“ **Arquivo terraform.tfvars (MÃ­nimo)**

```hcl
# === PROXMOX CONNECTION ===
proxmox_api_url          = "https://cacto.cefetes.br:8006/api2/json"
proxmox_api_token_id     = "root@pam!terraform"  
proxmox_api_token_secret = "SEU_TOKEN_AQUI"
proxmox_node             = "gardenia"

# === CLUSTER CONFIG ===
cluster_name = "k8s-cluster-viana"
environment  = "production"

# === NETWORK (CEFETES) ===
master_ips = ["172.17.176.34"]
worker_ips = ["172.17.176.35", "172.17.176.36"]

# === SSH SECURITY ===
ssh_public_key_path = "~/.ssh/k8s-cluster-key.pub"
vm_user            = "admviana"
```

---

## ğŸš€ **InstalaÃ§Ã£o Express**

### ğŸƒâ€â™‚ï¸ **MÃ©todo Ultra-RÃ¡pido**
```bash
# Clone + Setup + Deploy em uma linha
git clone <repo> && cd terraform-proxmox-k8s && make install
```

### ğŸ›ï¸ **MÃ©todo com Controle**
```bash
# 1. Preparar ambiente
make prerequisites

# 2. Inicializar Terraform  
make init

# 3. Revisar plano (opcional)
make plan

# 4. Aplicar infraestrutura
make apply

# 5. Verificar resultado
make validate
```

---

## ğŸ“Š **Resultado Esperado**

### ğŸ–¥ï¸ **Infraestrutura Criada**
```
âœ… 3 VMs Ubuntu 22.04 no Proxmox
   â”œâ”€â”€ k8s-cluster-viana-master-1  (172.17.176.34)
   â”œâ”€â”€ k8s-cluster-viana-worker-1  (172.17.176.35)  
   â””â”€â”€ k8s-cluster-viana-worker-2  (172.17.176.36)

âœ… Cluster Kubernetes v1.28.2
   â”œâ”€â”€ Control plane configurado
   â”œâ”€â”€ Workers unidos ao cluster
   â””â”€â”€ Flannel CNI funcionando

âœ… Rancher v2.7.5+ instalado
   â”œâ”€â”€ Interface web: https://172.17.176.34:8443
   â”œâ”€â”€ Credenciais: admin / admin123
   â””â”€â”€ cert-manager para certificados
```

### ğŸ”— **Pontos de Acesso**
| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **Rancher UI** | `https://172.17.176.34:8443` | `admin` / `admin123` |
| **Kubernetes API** | `https://172.17.176.34:6443` | Via kubeconfig |
| **SSH Master** | `ssh admviana@172.17.176.34` | Chave SSH |

---

## âœ… **VerificaÃ§Ãµes PÃ³s-Deploy**

### ğŸ” **Comandos de VerificaÃ§Ã£o**
```bash
# Status geral
make validate

# Conectividade SSH
make ping  

# Cluster Kubernetes
kubectl --kubeconfig=./kubeconfig get nodes

# Pods do sistema
kubectl --kubeconfig=./kubeconfig get pods -A

# Status do Rancher
kubectl --kubeconfig=./kubeconfig get pods -n cattle-system
```

### ğŸ“‹ **SaÃ­da Esperada**
```bash
$ kubectl get nodes
NAME                         STATUS   ROLES           AGE   VERSION
k8s-cluster-viana-master-1   Ready    control-plane   5m    v1.28.2
k8s-cluster-viana-worker-1   Ready    <none>          4m    v1.28.2  
k8s-cluster-viana-worker-2   Ready    <none>          4m    v1.28.2

$ kubectl get pods -n cattle-system
NAME                       READY   STATUS    RESTARTS   AGE
rancher-6f4c8c5d4b-xyz12   1/1     Running   0          3m
rancher-6f4c8c5d4b-abc34   1/1     Running   0          3m
rancher-6f4c8c5d4b-def56   1/1     Running   0          3m
```

---

## ğŸ› ï¸ **Comandos de GestÃ£o**

### ğŸ“Š **Monitoramento**
```bash
make status          # Status completo da infraestrutura
make logs           # Logs de deployment  
make check          # VerificaÃ§Ã£o rÃ¡pida
```

### ğŸ”§ **Acesso e ConfiguraÃ§Ã£o**
```bash
make ssh-master     # SSH no master node
make get-kubeconfig # Baixar kubeconfig
make rancher-info   # InformaÃ§Ãµes do Rancher
```

### ğŸ§¹ **ManutenÃ§Ã£o**
```bash
make clean-ssh-keys # Limpar known_hosts (VMs recriadas)
make destroy        # Destruir toda infraestrutura
make clean          # Limpar arquivos temporÃ¡rios
```

---

## ğŸ†˜ **Troubleshooting RÃ¡pido**

### âŒ **Problemas Comuns**

#### **"Template nÃ£o encontrado"**
```bash
# Verificar se template existe no nÃ³ correto
ssh root@gardenia "qm list | grep ubuntu-22.04-cloud"

# Se nÃ£o existe, criar:
./scripts/create-template.sh
```

#### **"VMs nÃ£o inicializam"**  
```bash
# Verificar recursos no Proxmox
pvesh get /nodes/gardenia/status

# Verificar logs
ssh root@gardenia "qm status <VMID>"
```

#### **"SSH nÃ£o conecta"**
```bash
# Limpar known_hosts
make clean-ssh-keys

# Verificar chave SSH
ssh-add ~/.ssh/k8s-cluster-key
```

#### **"Cluster nÃ£o forma"**
```bash
# Logs do kubelet
ssh -i ~/.ssh/k8s-cluster-key admviana@172.17.176.34 "sudo journalctl -u kubelet -f"

# Reexecutar Ansible se necessÃ¡rio
cd ansible && ansible-playbook -i inventory site.yml
```

### ğŸš‘ **Comandos de EmergÃªncia**
```bash
# Reiniciar tudo
make destroy && make install

# Reconfigurar apenas (sem destruir VMs)
cd ansible && ansible-playbook -i inventory site.yml

# Debug completo
make status && make logs
```

---

## ğŸ¯ **Casos de Uso EspecÃ­ficos**

### ğŸ« **Para LaboratÃ³rio/EducaÃ§Ã£o**
```bash
# ConfiguraÃ§Ã£o mÃ­nima de recursos
echo 'master_memory = 4096' >> terraform.tfvars
echo 'worker_memory = 8192' >> terraform.tfvars
echo 'environment = "development"' >> terraform.tfvars
```

### ğŸ¢ **Para ProduÃ§Ã£o**
```bash
# ConfiguraÃ§Ã£o robusta
echo 'master_memory = 16384' >> terraform.tfvars  
echo 'worker_memory = 32768' >> terraform.tfvars
echo 'worker_count = 5' >> terraform.tfvars
echo 'environment = "production"' >> terraform.tfvars
```

### ğŸ”— **Para IntegraÃ§Ã£o CI/CD**
```bash
# Usar variÃ¡veis de ambiente
export TF_VAR_proxmox_api_token_secret="token-from-ci"
export TF_VAR_cluster_name="k8s-ci-$(date +%Y%m%d)"

# Deploy automatizado
make install
```

---

## ğŸ‰ **PrÃ³ximos Passos**

### ğŸš€ **ApÃ³s InstalaÃ§Ã£o**
1. **Explore Rancher**: Apps, Projects, Users
2. **Deploy primeira app**: Via Rancher Apps Catalog
3. **Configure monitoring**: Prometheus + Grafana
4. **Setup backup**: Longhorn ou external
5. **Implemente CI/CD**: GitLab/Jenkins integration

### ğŸ“š **DocumentaÃ§Ã£o Adicional**
- `README.md` - DocumentaÃ§Ã£o completa
- `OVERVIEW.md` - VisÃ£o geral do projeto
- `BEST-PRACTICES.md` - Melhores prÃ¡ticas
- `docs/` - DocumentaÃ§Ã£o tÃ©cnica

---

<div align="center">

**âš¡ Seu cluster Kubernetes estÃ¡ pronto em minutos!**

ğŸš€ **Deploy** â†’ ğŸ”§ **Configure** â†’ ğŸ¯ **Use**

[![Powered by CEFET-ES](https://img.shields.io/badge/Powered%20by-CEFET--ES-blue)](https://cefetes.br)

</div>

## ğŸ“Š ConfiguraÃ§Ã£o PadrÃ£o

| Componente | IP | Recursos |
|------------|----|---------| 
| **Master** | 192.168.1.10 | 4 vCPU, 8GB RAM, 80GB |
| **Worker 1** | 192.168.1.20 | 4 vCPU, 16GB RAM, 50GB |
| **Worker 2** | 192.168.1.21 | 4 vCPU, 16GB RAM, 50GB |

## ğŸ› ï¸ Comandos Essenciais

### **GestÃ£o do Cluster**
```bash
make status           # Status geral
make logs            # Ver logs
make validate        # Validar configuraÃ§Ã£o
make get-kubeconfig  # Baixar kubeconfig
```

### **Acesso SSH**
```bash
make ssh-master      # Master node
make ssh-worker-1    # Worker 1
make ssh-worker-2    # Worker 2
```

### **ManutenÃ§Ã£o**
```bash
make clean-ssh-keys  # Limpar known_hosts
make destroy         # Destruir cluster
make clean           # Limpar arquivos temp
```

## ğŸ” VerificaÃ§Ãµes RÃ¡pidas

### **Status do Cluster Proxmox**
```bash
# Ver nÃ³s do cluster
pvecm nodes

# Templates por nÃ³
pvesh get /cluster/resources --type vm | grep template

# Storage disponÃ­vel
pvesh get /nodes/{node}/storage
```

### **Status do Kubernetes**
```bash
# NÃ³s do cluster
kubectl --kubeconfig=./ansible/kubeconfig get nodes -o wide

# Pods do sistema
kubectl --kubeconfig=./ansible/kubeconfig get pods -A

# Rancher status
kubectl --kubeconfig=./ansible/kubeconfig get pods -n cattle-system
```

## ğŸ“‹ DecisÃ£o por Tamanho de Cluster

| Tamanho | NÃ³s | EstratÃ©gia | Comando |
|---------|-----|------------|---------|
| **Pequeno** | 1-3 | Template em 1 nÃ³ | `./scripts/create-template.sh single` |
| **MÃ©dio** | 3-6 | Templates em 2-3 nÃ³s | `./scripts/create-template.sh auto` |
| **Grande** | 6+ | Storage compartilhado ou mÃºltiplos templates | `./scripts/create-template.sh node1,node2,node3` |

## âš¡ Quick Start

```bash
# 1. Criar template (escolha um mÃ©todo acima)
make create-template

# 2. Configurar nÃ³ no terraform.tfvars
proxmox_node = "your-node-here"

# 3. Executar projeto
make init
make apply

# 4. Acessar Rancher
make rancher-info
```

## ğŸ” VerificaÃ§Ãµes Ãšteis

### **Ver nÃ³s do cluster:**
```bash
pvecm nodes
```

### **Listar templates por nÃ³:**
```bash
# Em todos os nÃ³s
pvesh get /cluster/resources --type vm | grep template

# Em nÃ³ especÃ­fico
ssh root@{node} "qm list | grep template"
```

### **Verificar storage:**
```bash
pvesh get /nodes/{node}/storage
```

## âš ï¸ Pontos Importantes

### **1. Template + NÃ³ devem corresponder**
```hcl
# Se template estÃ¡ no "proxmox-node1"
proxmox_node = "proxmox-node1"  # âœ… Correto

proxmox_node = "proxmox-node2"  # âŒ Erro - template nÃ£o existe aqui
```

### **2. Storage considerations**
- **local-lvm**: Template fica local no nÃ³
- **shared storage**: Template disponÃ­vel em todos os nÃ³s
- **Verifique o tipo** do seu storage

### **3. DistribuiÃ§Ã£o de carga**
Para clusters maiores, considere:
- Criar templates em mÃºltiplos nÃ³s
- Distribuir VMs entre nÃ³s
- Usar storage compartilhado quando possÃ­vel

## ğŸ› ï¸ Troubleshooting

### **Template nÃ£o encontrado:**
```bash
# Verificar se template existe no nÃ³ correto
ssh root@$PROXMOX_NODE "qm list | grep ubuntu-22.04-cloud"

# Se nÃ£o existe, criar:
make create-template
```

### **NÃ³ nÃ£o acessÃ­vel:**
```bash
# Testar conectividade
ping {your-proxmox-node}
ssh root@{your-proxmox-node} "echo OK"

# Verificar se nÃ³ estÃ¡ no cluster
pvecm nodes
```

### **Storage insuficiente:**
```bash
# Verificar espaÃ§o
pvesh get /nodes/{node}/storage/{storage}/status

# Limpar templates antigos se necessÃ¡rio
qm destroy {template-id}
```

---

## ğŸ‰ Resumo

âœ… **Template pode ser criado em qualquer nÃ³**  
âœ… **Script automatizado facilita o processo**  
âœ… **ConfiguraÃ§Ã£o simples no terraform.tfvars**  
âœ… **Funciona em clusters de qualquer tamanho**  

**O importante Ã©:** Template criado â†” NÃ³ configurado no Terraform ğŸ¯
