# ğŸš€ Terraform Proxmox Kubernetes Cluster com Rancher

Este projeto provisiona um cluster Kubernetes no Proxmox VE usando Terraform e configura o cluster com Ansible, incluindo instalaÃ§Ã£o do Rancher para gerenciamento.

## âœ¨ CaracterÃ­sticas

- **ğŸ¯ IPs Fixos**: ConfiguraÃ§Ã£o com IPs fixos para maior controle e previsibilidade
- **ğŸš€ Ubuntu 22.04 LTS**: VersÃ£o estÃ¡vel e confiÃ¡vel com suporte estendido
- **âš¡ Kubernetes 1.28.2**: VersÃ£o estÃ¡vel e atual do Kubernetes
- **ğŸŒ Rancher 2.7.5**: Interface web completa para gerenciamento do cluster
- **ğŸ”§ AutomaÃ§Ã£o completa**: Do provisionamento Ã  configuraÃ§Ã£o, tudo automatizado
- **ğŸ“Š Cluster Proxmox**: Suporte completo para clusters Proxmox VE
- **ğŸ” SSH Key Authentication**: AutenticaÃ§Ã£o segura por chave SSH
- **ğŸ·ï¸ Tags Padronizadas**: OrganizaÃ§Ã£o com tags consistentes
- **âœ… ValidaÃ§Ãµes Robustas**: PrevenÃ§Ã£o de configuraÃ§Ãµes invÃ¡lidas
- **ğŸ“‹ Melhores PrÃ¡ticas**: ImplementaÃ§Ã£o seguindo padrÃµes de seguranÃ§a

## ğŸ“‹ ConfiguraÃ§Ã£o PadrÃ£o (Rede GenÃ©rica)

| Componente | IP Fixo | Recursos |
|------------|---------|----------|
| **Master** | 192.168.1.10 | 4 vCPU, 8GB RAM, 80GB |
| **Worker 1** | 192.168.1.20 | 4 vCPU, 16GB RAM, 50GB |
| **Worker 2** | 192.168.1.21 | 4 vCPU, 16GB RAM, 50GB |

### ğŸŒ Acessos do Cluster
- **Kubernetes API**: `https://192.168.1.10:6443`
- **Rancher UI**: `https://192.168.1.10:8443`
  - **UsuÃ¡rio**: admin
  - **Senha**: admin123
- **SSH Access**: `ssh -i ~/.ssh/k8s-cluster-key ubuntu@IP_DO_NO`

## ğŸ“‹ PrÃ©-requisitos

### 1. ğŸ–¥ï¸ Proxmox VE
- Proxmox VE 7.0+ instalado e configurado
- Template Ubuntu 22.04 Cloud-Init criado
- Token de API configurado com permissÃµes adequadas
- Recursos suficientes (mÃ­nimo: 8 vCPU, 40GB RAM, 180GB storage)

### 2. ğŸ› ï¸ Ferramentas Locais
- **Terraform** >= 1.0
- **Ansible** >= 2.12
- **Python** 3.8+
- **Git** para versionamento
- **SSH Client** configurado

### 3. ğŸ” AutenticaÃ§Ã£o SSH
Gere um par de chaves SSH dedicado para o cluster:

```bash
# Gerar chave SSH para o cluster
ssh-keygen -t rsa -b 4096 -f ~/.ssh/k8s-cluster-key -C "k8s-cluster@$(hostname)"

# Verificar se as chaves foram criadas
ls -la ~/.ssh/k8s-cluster-key*
```

**Importante**: O projeto estÃ¡ configurado para usar `~/.ssh/k8s-cluster-key` por padrÃ£o.
### 4. ğŸ“¦ Template Ubuntu 22.04
**Para cluster Proxmox:** O template pode ser criado em qualquer nÃ³, mas as VMs serÃ£o criadas no mesmo nÃ³ do template.

#### **OpÃ§Ã£o A: Script Automatizado (Recomendado)**
```bash
# Executar script no nÃ³ Proxmox ou com SSH configurado
./scripts/create-template.sh

# OpÃ§Ãµes disponÃ­veis:
# ./scripts/create-template.sh single          # Um nÃ³ apenas
# ./scripts/create-template.sh auto            # DetecÃ§Ã£o automÃ¡tica
# ./scripts/create-template.sh node1,node2     # NÃ³s especÃ­ficos
```

#### **OpÃ§Ã£o B: Manual**
Crie um template Ubuntu 22.04 com cloud-init no Proxmox:

```bash
# No Proxmox, criar template (executar no shell do Proxmox)
wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img
qm create 9000 --name ubuntu-22.04-cloud --memory 2048 --cores 2 --net0 virtio,bridge=vmbr0
qm importdisk 9000 jammy-server-cloudimg-amd64.img local-lvm
qm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0
qm set 9000 --boot c --bootdisk scsi0
qm set 9000 --scsi1 local-lvm:cloudinit
qm set 9000 --vga qxl
qm set 9000 --agent enabled=1
qm template 9000
```

**âš ï¸ Importante:** Anote o **nome do nÃ³** onde criou o template para configurar no `terraform.tfvars`.

### 5. ğŸ”‘ Token de API do Proxmox
Criar um token de API no Proxmox:

1. Acesse a interface web do Proxmox
2. VÃ¡ em Datacenter > Permissions > API Tokens
3. Crie um novo token para o usuÃ¡rio root@pam
4. Marque "Privilege Separation" como false

### 4. Chaves SSH
Gere um par de chaves SSH se ainda nÃ£o tiver:

```bash
ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa
```

### 5. Instalar dependÃªncias do Ansible
```bash
# Instalar coleÃ§Ãµes necessÃ¡rias
ansible-galaxy collection install -r ansible/requirements.yml

# Instalar dependÃªncias Python
pip3 install kubernetes
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. ğŸ“¥ Clonar e Configurar o Projeto

```bash
git clone <este-repositorio>
cd terraform-proxmox-k8s
chmod +x scripts/*.sh
```

### 2. ï¿½ InstalaÃ§Ã£o RÃ¡pida (Recomendada)

```bash
# Instalar prÃ©-requisitos e inicializar
make prerequisites
make init

# Configurar variÃ¡veis (editar conforme sua infraestrutura)
cp terraform.tfvars.example terraform.tfvars
nano terraform.tfvars

# Executar instalaÃ§Ã£o completa
make install
```

### 3. ğŸ¯ ConfiguraÃ§Ã£o Manual

#### **3.1. Configurar VariÃ¡veis do Terraform**
```bash
cp terraform.tfvars.example terraform.tfvars
```

Edite o arquivo `terraform.tfvars` com suas configuraÃ§Ãµes:

```hcl
# ========================================
# CONFIGURAÃ‡Ã•ES DO PROXMOX VE - OBRIGATÃ“RIO
# ========================================
proxmox_api_url          = "https://your-proxmox-server.domain.com:8006/api2/json"
proxmox_api_token_id     = "your-user@pve!your-token-name"
proxmox_api_token_secret = "your-token-secret-here"
proxmox_node             = "your-proxmox-node"

# ========================================
# CONFIGURAÃ‡Ã•ES DO AMBIENTE
# ========================================
environment = "production"  # development, staging, production

# ========================================
# CONFIGURAÃ‡Ã•ES DO CLUSTER
# ========================================
cluster_name = "my-k8s-cluster"
master_count = 1
worker_count = 2
template_name = "ubuntu-22.04-cloud"

# ========================================
# CONFIGURAÃ‡Ã•ES DE REDE
# ========================================
network_bridge  = "vmbr0"
network_gateway = "192.168.1.1"
dns_servers     = "8.8.8.8,8.8.4.4"
search_domain   = "local"

# IPs fixos para os nÃ³s
master_ips = ["192.168.1.10"]
worker_ips = ["192.168.1.20", "192.168.1.21"]

# ========================================
# CONFIGURAÃ‡Ã•ES DE SEGURANÃ‡A
# ========================================
ssh_public_key_path = "~/.ssh/k8s-cluster-key.pub"
vm_user             = "ubuntu"
vm_password         = "your-secure-password"  # Usado apenas como fallback

# ========================================
# CONFIGURAÃ‡Ã•ES DE HARDWARE
# ========================================
# Masters
master_memory    = 8192   # 8GB RAM
master_cpu       = 4      # 4 vCPUs
master_disk_size = "80G"

# Workers
worker_memory    = 16384  # 16GB RAM
worker_cpu       = 4      # 4 vCPUs
worker_disk_size = "50G"
```

**ğŸ” SeguranÃ§a**: Use variÃ¡veis de ambiente para tokens em produÃ§Ã£o:
```bash
export TF_VAR_proxmox_api_token_secret="seu-token-aqui"
```

## ğŸš€ ExecuÃ§Ã£o

### ğŸ¯ MÃ©todo Recomendado (Make)

```bash
# 1. Instalar dependÃªncias
make prerequisites

# 2. Inicializar Terraform
make init

# 3. Planejar execuÃ§Ã£o
make plan

# 4. Aplicar configuraÃ§Ã£o completa
make install
```

### ğŸ”§ MÃ©todo Manual (Terraform + Ansible)

#### **1. Inicializar Terraform**
```bash
terraform init
```

#### **2. Planejar a ExecuÃ§Ã£o**
```bash
terraform plan
```

#### **3. Aplicar a ConfiguraÃ§Ã£o**
```bash
terraform apply
```

#### **4. Aguardar e Configurar Cluster**
O processo irÃ¡:
1. âœ… Criar as VMs no Proxmox com SSH keys
2. âœ… Gerar inventÃ¡rio do Ansible automaticamente
3. âœ… Aguardar VMs inicializarem (60 segundos)
4. âœ… Executar playbooks Ansible:
   - Preparar sistemas operacionais
   - Instalar Docker em todos os nÃ³s
   - Instalar Kubernetes (kubeadm, kubelet, kubectl)
   - Configurar o master node com Flannel CNI
   - Adicionar workers ao cluster
   - Instalar cert-manager
   - Instalar e configurar Rancher

**â±ï¸ Tempo Estimado**: 15-20 minutos (dependendo da velocidade da internet e recursos)

## PÃ³s-instalaÃ§Ã£o

### 1. Verificar o cluster
```bash
# Usar Makefile
make check

# Ou manualmente
cd ansible && ansible-playbook -i inventory check-cluster.yml
```

### 2. Acessar o Rancher
ApÃ³s a instalaÃ§Ã£o, o Rancher estarÃ¡ disponÃ­vel em:
- URL: https://rancher.local (configurar no /etc/hosts)
- IP direto: https://<MASTER_IP>
- UsuÃ¡rio: admin
- Senha inicial: admin123

Para configurar o acesso:
```bash
# Adicionar ao /etc/hosts
echo "<MASTER_IP> rancher.local" | sudo tee -a /etc/hosts
```

### 3. Obter kubeconfig
### 3. Obter kubeconfig
ApÃ³s a aplicaÃ§Ã£o, copie o arquivo kubeconfig do master:

```bash
# Usar Makefile
make get-kubeconfig

# Ou manualmente
scp -i ~/.ssh/id_rsa ubuntu@<MASTER_IP>:~/.kube/config ./kubeconfig
```

### 4. Verificar o cluster

```bash
# Usar o kubeconfig baixado
kubectl --kubeconfig=./kubeconfig get nodes
kubectl --kubeconfig=./kubeconfig get pods -A
```

### 5. Conectar nas VMs

```bash
# Usar Makefile
make ssh-master

# Ou manualmente
ssh -i ~/.ssh/id_rsa ubuntu@<MASTER_IP>
ssh -i ~/.ssh/id_rsa ubuntu@<WORKER_IP>
```

## Estrutura do projeto

```
â”œâ”€â”€ main.tf                      # ConfiguraÃ§Ã£o principal do Terraform
â”œâ”€â”€ variables.tf                 # DefiniÃ§Ã£o de variÃ¡veis
â”œâ”€â”€ outputs.tf                   # Outputs do Terraform
â”œâ”€â”€ terraform.tfvars.example     # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ Makefile                     # Comandos automatizados
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ ansible/                     # ConfiguraÃ§Ã£o Ansible
â”‚   â”œâ”€â”€ site.yml                 # Playbook principal
â”‚   â”œâ”€â”€ inventory.tpl            # Template do inventÃ¡rio
â”‚   â”œâ”€â”€ ansible.cfg              # ConfiguraÃ§Ã£o Ansible
â”‚   â”œâ”€â”€ requirements.yml         # DependÃªncias Ansible
â”‚   â”œâ”€â”€ group_vars/
â”‚   â”‚   â””â”€â”€ all.yml              # VariÃ¡veis globais
â”‚   â””â”€â”€ roles/                   # Roles Ansible
â”‚       â”œâ”€â”€ common/              # PreparaÃ§Ã£o bÃ¡sica
â”‚       â”œâ”€â”€ docker/              # InstalaÃ§Ã£o Docker
â”‚       â”œâ”€â”€ kubernetes/          # InstalaÃ§Ã£o Kubernetes
â”‚       â”œâ”€â”€ kubernetes-master/   # ConfiguraÃ§Ã£o master
â”‚       â”œâ”€â”€ kubernetes-worker/   # ConfiguraÃ§Ã£o workers
â”‚       â””â”€â”€ rancher/             # InstalaÃ§Ã£o Rancher
â””â”€â”€ scripts/                     # Scripts auxiliares
    â”œâ”€â”€ check-cluster.sh         # Verificar cluster
    â””â”€â”€ deploy-example.sh        # Deploy de exemplo
```

## ğŸ› ï¸ Comandos Ãšteis (Makefile)

```bash
# ===== INSTALAÃ‡ÃƒO =====
make prerequisites      # Instalar dependÃªncias
make init              # Inicializar Terraform
make plan              # Planejar mudanÃ§as
make install           # InstalaÃ§Ã£o completa
make apply             # Aplicar apenas Terraform

# ===== VERIFICAÃ‡ÃƒO =====
make check             # Verificar cluster
make validate          # Validar configuraÃ§Ã£o
make get-kubeconfig    # Baixar kubeconfig

# ===== ACESSO SSH =====
make ssh-master        # Conectar no master
make ssh-worker-1      # Conectar no worker 1
make ssh-worker-2      # Conectar no worker 2

# ===== MANUTENÃ‡ÃƒO =====
make clean-ssh-keys    # Limpar chaves SSH conhecidas
make logs              # Ver logs do deployment
make status            # Status dos recursos

# ===== DESTRUIÃ‡ÃƒO =====
make destroy           # Destruir infraestrutura
make clean             # Limpar arquivos temporÃ¡rios
```

### ğŸ“Š Comandos de Monitoramento

```bash
# Status dos nÃ³s
kubectl --kubeconfig=./ansible/kubeconfig get nodes -o wide

# Pods do sistema
kubectl --kubeconfig=./ansible/kubeconfig get pods -A

# Status do Rancher
kubectl --kubeconfig=./ansible/kubeconfig get pods -n cattle-system

# Recursos do cluster
kubectl --kubeconfig=./ansible/kubeconfig top nodes
```

## PersonalizaÃ§Ã£o

### Alterar recursos das VMs
Edite as variÃ¡veis no `terraform.tfvars`:

```hcl
# Master nodes
master_memory    = 4096  # MB
master_cpu       = 2
master_disk_size = "50G"

# Worker nodes
worker_memory    = 8192  # MB
worker_cpu       = 4
worker_disk_size = "100G"
```

### Adicionar mais workers
Altere a variÃ¡vel `worker_count`:

```hcl
worker_count = 3  # Para 3 workers
```

### Configurar rede diferente
Ajuste as configuraÃ§Ãµes de rede:

```hcl
network_cidr    = "10.0.0.0/24"
network_gateway = "10.0.0.1"
```

## Troubleshooting

### VMs nÃ£o inicializam
- Verifique se o template existe no Proxmox
- Confirme se o nome do nÃ³ estÃ¡ correto
- Verifique se hÃ¡ recursos suficientes

### Erro de SSH
- Confirme se a chave SSH estÃ¡ correta
- Verifique se as VMs tÃªm acesso Ã  internet
- Confirme se o cloud-init estÃ¡ funcionando

### Cluster nÃ£o forma
- Verifique logs: `journalctl -u kubelet`
- Confirme conectividade entre nÃ³s
- Verifique se as portas necessÃ¡rias estÃ£o abertas

### Comandos Ãºteis

```bash
# Ver status das VMs no Proxmox
pvesh get /cluster/resources --type vm

# Logs do cloud-init nas VMs
sudo cat /var/log/cloud-init-output.log

# Status do kubelet
sudo systemctl status kubelet

# Logs do kubelet
sudo journalctl -u kubelet -f
```

## Limpeza

Para destruir toda a infraestrutura:

```bash
terraform destroy
```

## Componentes instalados

- **Docker**: Container runtime
- **Kubernetes 1.28**: Orquestrador de containers
- **Flannel**: Plugin de rede para pods
- **containerd**: Container runtime interface
- **cert-manager**: Gerenciamento de certificados
- **Rancher**: Plataforma de gerenciamento Kubernetes

## Rancher - Funcionalidades

O Rancher fornece:
- Interface web para gerenciar clusters Kubernetes
- GestÃ£o de usuÃ¡rios e permissÃµes
- Monitoramento e alertas
- CatÃ¡logo de aplicaÃ§Ãµes
- Backup e restore
- GestÃ£o de projetos e namespaces

## Troubleshooting

### VMs nÃ£o inicializam
- Verifique se o template existe no Proxmox
- Confirme se o nome do nÃ³ estÃ¡ correto
- Verifique se hÃ¡ recursos suficientes

### Erro de SSH/Ansible
- Confirme se a chave SSH estÃ¡ correta
- Verifique se as VMs tÃªm acesso Ã  internet
- Confirme se o cloud-init estÃ¡ funcionando
- Teste conectividade: `ansible all -m ping`

### Cluster nÃ£o forma
- Verifique logs: `journalctl -u kubelet`
- Confirme conectividade entre nÃ³s
- Verifique se as portas necessÃ¡rias estÃ£o abertas

### Rancher nÃ£o acessa
- Verifique se todos os pods estÃ£o rodando: `kubectl get pods -n cattle-system`
- Confirme se cert-manager estÃ¡ funcionando: `kubectl get pods -n cert-manager`
- Verifique logs do Rancher: `kubectl logs -n cattle-system -l app=rancher`

### Comandos Ãºteis

```bash
# Verificar status geral
kubectl get nodes
kubectl get pods -A

# Logs especÃ­ficos
kubectl logs -n kube-system -l app=flannel
kubectl logs -n cattle-system -l app=rancher

# Reiniciar Rancher
kubectl rollout restart deployment/rancher -n cattle-system

# Ver status das VMs no Proxmox
pvesh get /cluster/resources --type vm

# Logs do cloud-init nas VMs
sudo cat /var/log/cloud-init-output.log

# Status do kubelet
sudo systemctl status kubelet

# Executar playbook especÃ­fico
cd ansible && ansible-playbook -i inventory roles/rancher/tasks/main.yml
```
